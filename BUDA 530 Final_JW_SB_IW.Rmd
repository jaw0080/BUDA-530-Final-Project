---
title: "BUDA 530 Final_JW_SB_IW"
author: "Jordon Wolfram, Ivonne Wardell, and Scott Branham"
date: "February 24, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Instructions
**This final is to be done in your groups.  Only one document is needed per group.  Some questions require code, and the analysis of data, some questions check the concepts that we have discussed in this course.  There are a total of 5 questions on this final.  Please submit your .Rmd and word/pdf document via e campus.  The due date will be announced on e campus and via email.**

### Problem 1 (20 points)
**In your own words address the following questions about maximum likelihood estimation.  What is maximum likelihood estimation?  What is the procedure for finding the maximum likelihood estimates?  Is there a relationship between the ordinary least squares estimates and generalized linear models that is based on maximum likelihood estimation?  If so give insights on that relationship.**


Maximum likelihood has been at the foundation of this BUDA530 class.  The data that were are looking at is essentially random variables and we have to review this as data analyists and be able to quantify the randomness.  Maximum likelihood estimator (MLE) is a useful tool in finding the likelihood of the parameter given the data that will best allow us to model or fit it.  This is optimized by finding the ideal \theta value that "maximizes" the closeness.

One standard practice in MLE is using the log likelihood.  It is still looking for the MLE, but log functionality surpresses the data into a form that can identify the density within the data better.  As this affects the results of the predictor and response variables in ratio, it is mathmatically allowable, although when interpreting coefficients, it should be considered that the results may need to be transformed back before being interpreted.  Finding the MLE can be done using calculus on simple data as we showed in HW1, but with more complex data, we generally use computing methods to find this.

Ordinary Least Squares (OLS), Generalized Linear Models (GLM) and MLE are related to each other.  To understand the relationship, I would add one more term, "Linear Regression".  OLS finds the estimator using linear regression parameters that are unbiased and chooses this by minimizing the sum of the squares.  It is fitting the existing data as well as it can on a line.  Because of this, OLS operates best when all of the data is available and interprets from that.  The function lm() utilizes this and it is very prevelent.

GLMs, on the other hand, can use non-normal data which is important for binonmials, Poisson, and more options that we have used.  While it can handle normal data, it can also break away from the norm when needed by using MLE methodology as its regression parameter.  Unlike OLS, MLE is finding the parameter that maximizes the probability of matching the sample data.  The "link" inside of the glm() function defaults to "logit" for the advantages about using log likelihood that were discussed already.

Linear Regression and OLS go together in the same way that GLM and MLE go together.  OLS and MLE are methods for fitting data best through linear modeling.  This relationship can be specifically seen in that if the actual data has a normal distribution around the mean, then mathmatically, MLE = OLS.  When data is not normal, it becomes important to understand the differences between the two for choosing the methodology that will allow you to model the data best.



### Problem 2 (20 points)
**Here are some  common problem occurring in industry right now.  Part of your education is to be able to analyze data, but also start to think about the type of analysis needed to answer the question. Most of your clients will never see what happens behind the scenes but they will be impacted by the results. As much as we would like it, most stakeholders don't know the analysis needed they just know the question they need an answer too.  Below I'm going to describe 2 scenarios.  Suggest the type of analysis/model you would used based on the methods you learned in this course.  Give justification for your selection.**

**A company that offers lines of credit (think bank or credit cards) is interested in the risk associated with their customers.  There are many different ways to view this, but they plan to use this information to offer better rates, or make offers to new customers based on information they have on these customers.  The information available is income, employment history, home ownership, "some" credit history, and current credit liability for a credit report.  We have credit score, but are looking to supplement it with our own metric and recommendations.  We believe that using credit score only may be too restrictive to customers who will pay appropriately.  We are actively looking at the risk associated with these lines of credit.   For right now these loans are for a fixed credit limit.**

####Scenario 1  
**One stakeholder is interested in different levels of risk of default/non-payment.  They have provided information on 6 levels of risk, of every individual for their opinion and would like you to create a model based on this.  The 6 levels of risk, are "extremely low risk", "low risk", "moderate risk", "high risk", and "extremely high risk".  They ask you to find relevant predictors for this metric and report a model they can use in an automated procedure to provide these recommendations on top of what they already have.  Recommend a statistical model that you can use and how you would evaluate this model relative to customer needs.  Are there issues with the method you've proposed?  How can you explain this to your client?**

•	My initial thought was ordinal regression, but I am unsure if this scenario fits the definition: “used for a variable whose value exists on an arbitrary scale where only the relative ordering between different values is significant”
1)	Does the risk scale seem arbitrary or more well-defined?
2)	Is the ordering more than just relative (I might be overthinking this)
•	GLM may be a better fit here, but I’m honestly not sure. To me it depends on:
1)	Can we firmly establish a relationship between the mean and variance at each risk level?
2)	If so, I lean towards Poisson for this scenario if – and only if – we can treat “events” as equal regardless of level of risk, i.e. moderate risk and high risk have the same weight in the model. If not we would have to determine a proper weighting before creating the Poisson distribution. 


####Scenario 2  
**Another stakeholder is interested in only defaults.  Since credit is offered to those who the company deems appropriate to offer credit to there is a certain "filter" on who will default.  Basically defaults become a "rare" occurrence, but we still want to be able to predict.  The data the company provides shows a 15% default rate, which they deem acceptable, but they want to see if they can identify contributory factors to defaults.  Prediction at an individual level is done through another means.  They want you to develop a statistical model that can identify the number of defaults that can occur in a specific month.  They also suggest there could be a time effect (month may matter) and number of defaults the previous month may matter as well.  Explain the model you would try to implement and why it makes sense.  How would you evaluate this model and method?  What are the issues that can arise with your approach?  How would you explain this to your client?**

•	Two Phase Approach
1)	Default / non-payment makes me think binomial regression and distribution model is the way to go. There are really only two possible outcomes of the variable (did the customer default on their payment or not?) based on the 6 independent variables mentioned in the scenario
	We could run models and look at the Log Odds ratios to determine initial variability, then conduct inference and model testing to check the overall deviance levels
	The end product would be an overall risk matrix that lists the dependent and independent variables, then shows a decision on whether we recommend issuing credit to a particular customer
2)	Time Series Analysis
	Utilize the ARMA model to look for default instances that may be one-time default occurrences, seasonality (e.g. somebody misses a payment during the holiday season), or other activity
	Create an ARIMA model to determine if there is an overall trend in financial activities by customer to determine risk and utilize autocorrelation within that model 
	The end product would be a confidence model that defines thresholds of risk based on what we see over time and recommends a go / no-go decision to the stakeholder
	Reference financial data plot from TSAR document (pg. 7)



### Problem 3 (20 points)
**In this course we've discussed generalized linear models, time series analysis, the non-linear models.  In your own words and in less than a page describe how these three concepts are relate to one another?  If there is no relationship between these methods explain why they are not connected?**

Generalized Linear Models (GLMs), Non-Linear Models (NLMs), and Time Series Models (TSMs) are forms of regression analysis that are related in that all three are developed to predict the probability of an event that occurred, as related to a dependent variable, was indeed significant.  Thus, it provides a better understanding of how the data of interest is behaving overall. This enhanced insight of the data’s behavior helps decision makers and/or end users to then forecast those events that are statistically proven to most likely occur – or not occur - in the future and hopefully improve the end state that is desired.

Additionally, all three models depend on a defined data structure to enable the possibility of fitting the output in the most appropriate way to be impactful. For GLMs, the structure includes linear predictors and a relationship between the mean and variance of the population; for NLMs, the structure includes determining the proper model to use and requires starting values – typically in the form of minimums or maximums – before transformation can occur; and for TSMs the data must be defined at fixed intervals and typically in a specific order prior to conducting analysis of any kind. 

The three methods of interest do lack relation on a couple of key points that differentiate their potential utility for being utilized as a best-fit model for data behavior or prediction. The biggest of these is obvious in that NLMs do not require the relationship between variables nor does the data have to be linear in nature. Unlike linear regression and time series analysis, non-linear functions can have more than one parameter per predictor variable. This enhanced level of variation fractures NLM’s relationship to the other two regression models. It does afford the data scientist with far greater flexibility in using NLMs to determine the best fit possible for the data population. However, that flexibility takes away the ability to have one true hypothesis test that can be measured via P-value that is so commonly used in regression models to predict the validity of a trend or assumption. 


### Problem 4 (20 points)
**In this problem we want to study the labor participation of women in the 1980's.  The `Mroz` data in the `car`package depicts labor force participation.  Look at the help file for definition of the variables.  We are going to use `lfp` as our response in this case.  Create and decide on a model (you need to fit a few), that models this, discuss relevant variables and the impact they have.  You can use lwg in your model as it depicts what income would be expected if that individual worked. At least one of your models must, use splines or non-linear terms.**  

Start by reviewing the variables for the 753 observations of data.
```{r}
library(car)
library(splines)
library(mgcv)
data("Mroz")
head(Mroz)
help(Mroz) #help define the data
summary(Mroz) #overview of the data
sapply(Mroz, class) #see the variables classes
```
There are 325 observations where married women were not in the labor force and 428 observations where they were.  The response in our model is going to be a factor of "yes" or "no" for lfp.  The data is quite tidy and does not have "NA" values that need to be worked around.  Wewe need to build a non-linear model that can consider the variables and make a prediction about whether lfp will be "yes" or "no"


####Build and hone basic models
```{r}
##start by looking at a linear model with a binomial response
modLFP1 <- glm(lfp ~ k5 + k618 + age + wc + hc + lwg + inc, data = Mroz, family = binomial(link = "logit"))

#now the non-linear, starting with polynomials:
##I tried all of the numeric values to different degrees of polynomials and used AIC to judge fit.  The model below has the best polynomial results that I could put together
modLFP2 <- glm(lfp ~ poly(k5,1,raw=FALSE) + poly(k618,1,raw=FALSE) + poly(age,1,raw=FALSE) + wc + hc + poly(lwg,2,raw=FALSE) + poly(inc,2,raw=FALSE), data = Mroz, family = binomial(link = "logit"))

#now compare each variable with a spline instead to see what can be improved.  Overall the splines improved the fit from the linear model, but not quite as well as the polynomial did.  One again, many variations of the split model below were tried before choosing this one.
modLFP3 <- glm(lfp ~ k5 + k618 + age + wc + hc + bs(lwg, knots =1)  + bs(inc,knots = c(25,50,75)), data = Mroz, family = binomial(link = "logit"))

#create a GAM.  By adjusting the values of this and reviewing the AIC, the model below fit best.
modLFP4 <- gam(lfp ~ k5 + k618 + s(age) + wc + hc + s(lwg) + s(inc), data = Mroz, family = binomial(link = "logit"))



AIC(modLFP1,modLFP2, modLFP3, modLFP4)
summary(modLFP4)
```


Predict using the model and review how well it predicts the lfp.
```{r}
predLFP <-predict(modLFP4,type="response", newdata=Mroz)
predLFP1 <- ifelse(predLFP>=0.5, "Yes", "No")

table(predLFP1) #what the model predicts
table(predLFP1,Mroz$lfp) #This will show how many times the model predicted right against the actual data.

correctPreds <- as.numeric(length(predLFP1)) - (sum(table(predLFP1,Mroz$lfp))-sum(diag(table(predLFP1,Mroz$lfp)))) #How many predictions are correct
print(correctPreds)

print(correctPreds / length(predLFP1)) 

```

The model that we fit was able to predict 571 out of the 753 times (75%) correctly whether a married woman within this survey would be part of the labor force.  From the summary, it is important to review the coefficients of the variables.  K618 (how many children 6-18 years old) and whether or not either gender went to college was not found to be significant within the model.  K5 (how many children 5 years old or younger) was significant in the model with a linear trend interfering the higher number of children in this age bracket make it less likely for a woman to be in the workforce.

Age, lwg, and income were significant as well, but these trends were non-linear and so they lose simple interpretability.  To compensate for this, they are plotted individually below, but using the GAM of the model.

```{r}
library(faraway)
#Overview
plot(modLFP4, residuals=T, pch=19, cex=0.75,scheme=1, col=3 , shade=T, shade.col='gray')

#Plot model for age
modLFPAge <- gam(lfp ~ s(age), data = Mroz, family = binomial(link = "logit"))
plot(predLFP~age,data = Mroz)
x <-seq(30,60,1)
lines(x,ilogit(predict(modLFPAge,newdata=data.frame(age=x))),col=2)


#Plot model for lwg
modLFPLwg <- gam(lfp ~ s(lwg), data = Mroz, family = binomial(link = "logit"))
plot(predLFP~lwg,data = Mroz)
x <-seq(0,2.4,0.1)
lines(x,ilogit(predict(modLFPLwg,newdata=data.frame(lwg=x))),col=2)


#Plot model for inc
modLFPInc <- gam(lfp ~ s(inc), data = Mroz, family = binomial(link = "logit"))
plot(predLFP~inc,data = Mroz)
x <-seq(0,60,1) #not using full data range as it becomes too sparse
lines(x,ilogit(predict(modLFPInc,newdata=data.frame(inc=x))),col=2)
```


These plots help to visualize the non-linear trends within the model by breaking down the response trend based on a single variable.
-Age stays pretty  steady from 30 to 40 and then it starts to become less likely that the the married woman will participate in the workforce as their age increases
-Lwg is hard to interpret but appears participatants in the survey with "average" levels of income potential were less likely to work.
-Inc shows that as the family income increases, the likelyhood of participating in the workforce decreases.




### Problem 5 (20 points)
**In the `mlogit` package there is a data set called `Fishing`.  This data depicts a customer choice model of different recreational fishing choices.  This is based on price, how many fish are caught, and income.   Create a model based on price, catch, and inocme, that tells me about a customers choice of mode.  Describe how you arrived at your model and any insights it provides.**

```{r echo = FALSE, warning = FALSE, message = FALSE}
#install.packages("mlogit")
#install.packages("groupdata2")
#install.packages("corrplot")

library(mlogit); library(tidyr); library(dplyr); library(stargazer); library(ggplot2)
library(groupdata2) #for partitions and generate folds 
library(corrplot) #for correlation plots
library(nnet)

```


```{r}
?Fishing

data("Fishing")
head(Fishing)
```
 
This problem requires us to classify the mode of recreational fishing given the predictor variables. We have 4 choices for modes of recreation fishing: beach, pier, boat and charter (the choice); 2 alternative variables: price and catch rate for each mode (the alternatives); and income which indicates each individual's monthly income.  

We check the choices (response variable). Charter appears to be the preferred choice with the highest number of individuals. 
```{r}
#check the dependent variable
table(Fishing$mode)
```

The data is in `wide` format; to use the mlogit function we first need to convert it into a `long` format
```{r}
fish_long <- mlogit.data(Fishing, shape = "wide", varying = 2:9, choice = "mode")
head(fish_long, 20)
```
In the `long` format we see that individuals are labeled `chid` or choice ID and each individual has 4 choices of mode. The data varies by both individuals and the fishing mode. The income does not vary by individual. `mode` represents the choice actually made by the individual. 

*Exploration of the data* 


```{r}
#summary of the fish_long data
summary(fish_long)
```

```{r}
#calculate proportion of mode choices 
exp <- group_by(fish_long, mode, alt) %>% 
        summarise(count = n()) %>% 
        mutate(etotal = sum(count), proportion = count/etotal)

#plot 
ggplot(exp, aes(x = alt, y = proportion, group = mode, linetype = mode, col = mode)) + 
        geom_line() + 
        ylim(0, 0.4) +
        ggtitle("Proportion of recreation mode, one of: beach, pier, boat and charter ") +
        xlab("Choice") + ylab("Proportion")

```

  
The actual choice made by an individual is given by the column `mode` as being TRUE, for example, the choice for individual `chid` 1 was `charter` and the choice for individual `chid` 3 was boat. From the plot we see that charter is the most chosen mode of recreational fishing with almost 40% of the total, followed by boat, pier and beach last with just over 10%. Conversely, we could have said that beach is the least selected alternative for recreational fishing, followed by pier, boat and charter and these correspond to the FALSE `mode`. 

We can look at the price and catch rate distributions for each mode
```{r}
#density plot for price
ggplot(fish_long, aes(x = price, col = alt)) + 
        geom_density() + 
        ggtitle("Price distribution for each mode choice") +
        xlab("Price") + ylab("Density")

#density plot for catch rate
ggplot(fish_long, aes(x = catch, col = alt)) + 
        geom_density() + 
        ggtitle("Catch rate distribution for each mode choice") +
        xlab("Catch rate") + ylab("Density")
```
  
We can make some interesting observations from the price density plots: first, we see skewed distributions for all the options, and all options have about the same range of prices although one can find more cheaper options for boat and charter, which makes sense since these are the most popular options (as we saw from the proportion plot). 

The catch rate density plots are less insightful but we can see that charter has the widest and the highest catch rate and it's a pretty uniform distribution. 

There doesn't seem to be a correlation between price and catch rate and we can verify by looking at a correlation plot:
```{r}
#subset the numerical variables only 
fish_num <- names(fish_long)[c(2, 4, 5)]
fish_num <- subset(fish_long, select = fish_num)

#calculate and plot correlations
m <- cor(fish_num)
corrplot(m, method = "number", type = "upper")
```
  
As suspected, there isn't much correlation between price and catch rate but we see a strong correlation between price and income. 

```{r}
#plot income vs price
ggplot(fish_long, aes(x = income, y = price, col = alt)) +
        geom_point() +
        geom_smooth() +
        ggtitle("Income vs price of mode choice") +
        xlab("Income") + ylab("Price")
```
  
From the plot we can see that as individual income increases, individuals are willing to pay higher prices for the mode of recreation. Individuals are willing to pay the most for pier even though the catch rate for this choice is much lower compared to charter or beach. 

```{r}
filter(fish_long, mode == TRUE) %>% 
        ggplot(aes(x = alt, y = income, col = alt)) + 
        geom_boxplot() +
        ggtitle("Income for each one of mode choices") +
        xlab("Mode choice") + ylab("Income")
```
  
Finaly we dont see a difference in income ranges for each of the mode choices.

We next summarize some of the data: 
```{r}
filter(fish_long, mode == TRUE) %>% 
        group_by(mode = alt) %>% 
        summarise(count = n(), min_price = min(price), max_price = max(price), med_price = median(price), mean_income = mean(income)) 

filter(fish_long, mode == TRUE) %>% 
        group_by(mode = alt) %>% 
        summarise(count = n(), min_catch = min(catch), max_catch = max(catch), med_catch = median(catch), mean_income = mean(income)) 
```

####Data analysis

We will first use the mlogit package to model multinomial logit models. 

```{r}
#start with the price and catch predictors and without the intercepts
m <- mlogit(mode ~ price + catch | 0, fish_long)

#model summary
summary(m)
```
The coefficient for price is negative, meaning that as the cost of the choice goes up keeping the catch rate the same, the probability of selecting the mode goes down. The coefficient for catch is positive indicating that as the rate of catch goes up, with price remaining constant, the probability of selecting that mode goes up. 

Both coefficients are significant with `p-value` being much smaller than 0.05

We compare the frequencies of the actual alternatives with frequency of model estimates:
```{r}
#calculate and extract both frequencies
freq_actual <- m$freq/sum(m$freq)
freq_model <- apply(fitted(m, outcome=FALSE), 2, mean)

#rbind
rbind(freq_actual, freq_model)
```
The model predictions are somewhat off from the actual frequencies of mode. The model is correctly predicted that `boat` and `charter` are the most selected choices although the model predicts that `boat` is more popular than `charter` which is not the case for actuals.


```{r}
#calculate the ratio of coefficients 
coef(m)["catch"]/coef(m)["price"]
```
The ratio of catch to price coefficient indicates that everything else being the same, individuals are willing to forgo 47% of the catch rate for a 1 unit reduction of mode price. This seems high. 

We will add the intercepts to see if it changes the model results:

```{r}
#with the intercepts
m1 <- mlogit(mode ~ price + catch, fish_long)

#model summary
summary(m1)
```
```{r}
#calculate and extract both frequencies
freq_model_m1 <- apply(fitted(m1, outcome=FALSE), 2, mean)

#rbind
rbind(freq_actual, freq_model_m1)
```

We see that the frequencies match exactly indicating an improvement to the model

```{r}
#calculate the ratio of coefficients 
coef(m1)["catch"]/coef(m1)["price"]
```
The ratio of catch to price coefficient indicates that everything else being the same, individuals are willing to forgo 15% of the catch rate for a 1 unit reduction of mode price. This looks more reasonable than the previous estimate of 47%

We will now add the income to the model. We will create 2 models: one with cost/income to model the fact that higher income individuals are less concerned with higher prices and vice-versa; and one with income effects on the alternatives 
```{r}
#model for cost/income 
m2 <- mlogit(mode ~ price + catch + I(price/income), fish_long)
summary(m2)
```
```{r}
#model for income specific effects 
m3 <- mlogit(mode ~ price + catch | income, fish_long)
summary(m3)
```

The model with price/income was worse than our previous model where income was not taken into account even though the price/income is highly significant. The income specific model is an improvement and suggests that as income goes up the probability of boat choice goes up relative to the other alternatives. Also as income goes up the probability of the other choices goes down. The income terms are not significant indicating that monthly income doesn't have an effect on the choices of mode. This may be due to the fact that individuals may save portion of their income for recreational fishing regardless of how much monthly income they make. 

We compare the models
```{r}
#likelihood test 
lrtest(m3, m1)

#Wald test 
waldtest(m3, m1)

#score test 
scoretest(m3, m1)
```
All tests indicate that we should use the model with the income effect. 
```{r}
#convert the coefficients to odds 
exp(coefficients(m3)[4:8])

```
* For one unit increase in income, the odds of selecting charter and pier go down and the odds of selecting boat over beach go up.  
* For one unit increase in price, the odds of choosing a mode goes down by 0.98 assuming catch remains the same across all choices
* And for one unit increase in catch rate, the odds of choosing a mode goes up by 1.43 assuming price remains the same across all choices


```{r}
#prediction vs actuals --- was unable to create a contingency matrix
predictions <- predict(m3)
actuals <- m3$freq/sum(m3$freq)

rbind(predictions, actuals)
```

We will now try a multinomial logit model using the multinom() function. We will follow Faraway's procedure

```{r}
#fit the model using all the relevant predictors
m4 <- multinom(alt ~ price + catch + income, fish_long)
```

We then use AIC to select which predictors add information to the model 
```{r}
#use the step function 
m5 <- step(m4)
```
The step function suggests that keeping all predictors results in the best AIC model, indicating that all variables are significant

```{r}
#model summary
summary(m4)
```
The summary output has the coefficients and the standard errors. The coefficients are being compared to the baseline alt = beach.  
* A one unit increase in monthly income increases the log odds of choosing boat and charter over beach, but decreases the log odds of choosing pier over beach. The coefficients are quite small though which is consistent with our previous observation that income ranges are about the same for all mode choices
* A one unit increase in price decreases the log odds of choosing beach over boat and charter and increases the log odds of selecting beach over pier - we saw in the density plots that pier prices remain higher than all other options for price > 150 so this result makes sense  
* Catch rate has the highest coefficients and we can see why charter is the most popular choice since a one unit increase in catch rate increases the log odds of choosing charter over beach by 2 units  
* We can infer from the model coefficients that catch rate is the biggest factor in selection of mode of recreational fishing, followed by price and then income. We previously saw that charter is the most popular mode and we can confirm from the model that the higher proportion of charter choice is significant

We will now compare the predicted values from the model with the actuals 

```{r}
#prediction vs actuals 
xtabs(~ predict(m4) + fish_long$alt)
```
```{r}
#calculate the proportion of correct classifications 
(281 + 696 + 657 + 522) / nrow(fish_long)
```
Only 46% of the data is correctly classified based on fitting the entire data set. Had we split the data into train and test subsets, we could expect the performance to be slightly worse since the model would be predicting new data. According to Faraway this performance is not unexpected when using multinomial logit models for classification problems. 


