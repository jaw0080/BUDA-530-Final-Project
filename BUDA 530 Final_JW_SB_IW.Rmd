---
title: "BUDA 530 Final_JW_SB_IW"
author: "Jordon Wolfram, Ivonne Wardell, and Scott Branham"
date: "February 28, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Problem 1 (20 points)
**In your own words address the following questions about maximum likelihood estimation.  What is maximum likelihood estimation?  What is the procedure for finding the maximum likelihood estimates?  Is there a relationship between the ordinary least squares estimates and generalized linear models that is based on maximum likelihood estimation?  If so give insights on that relationship.**


Maximum likelihood has been at the foundation of this BUDA530 class.  The data that were are looking at is essentially random variables and we have to review this as data analysts and be able to quantify the randomness.  Maximum likelihood estimator (MLE) is a useful tool in finding the likelihood of the parameter given the data that will allow us to best fit the model.  This is optimized by finding the ideal \theta value that "maximizes" the closeness.

One standard practice in MLE is using the log likelihood.  It is still looking for the MLE, but log functionality surpresses the data into a form that can identify the density within the data better.  As this affects the results of the predictor and response variables in ratio, it is mathmatically allowable, although when interpreting coefficients, it should be considered that the results may need to be transformed back before being interpreted.  Finding the MLE can be done using calculus on simple data as we showed in Module Homework 1, but with more complex data, we generally use computing methods to find this.

Ordinary Least Squares (OLS), Generalized Linear Models (GLM) and MLE are related to each other.  To understand the relationship, I would add one more term: Linear Regression.  OLS finds the estimator using linear regression parameters that are unbiased and chooses this by minimizing the sum of the squares.  It is fitting the existing data as well as it can on a line.  Because of this, OLS operates best when all of the data is available and interprets from that.  The function lm() utilizes this and it is very prevelent.

GLMs, on the other hand, can use non-normal data which is important for binomials, Poisson, and more options that we have used.  While it can handle normal data, it can also break away from the norm when needed by using MLE methodology as its regression parameter.Unlike OLS, MLE is finding the parameter that maximizes the probability of matching the sample data.  The "link" inside of the glm() function defaults to "logit" for the advantages about using log likelihood that were discussed already.

Linear Regression and OLS go together in the same way that GLM and MLE go together. OLS and MLE are methods for fitting data best through linear modeling.  This relationship can be specifically seen in that if the actual data has a normal distribution around the mean, then mathmatically, MLE = OLS.  When data is not normal, it becomes important to understand the differences between the two for choosing the methodology that will allow you to model the data best.


### Problem 2 (20 points)
**Here are some  common problem occurring in industry right now.  Part of your education is to be able to analyze data, but also start to think about the type of analysis needed to answer the question. Most of your clients will never see what happens behind the scenes but they will be impacted by the results. As much as we would like it, most stakeholders don't know the analysis needed they just know the question they need an answer too.  Below I'm going to describe 2 scenarios.  Suggest the type of analysis/model you would used based on the methods you learned in this course.  Give justification for your selection.**

**A company that offers lines of credit (think bank or credit cards) is interested in the risk associated with their customers.  There are many different ways to view this, but they plan to use this information to offer better rates, or make offers to new customers based on information they have on these customers.  The information available is income, employment history, home ownership, "some" credit history, and current credit liability for a credit report.  We have credit score, but are looking to supplement it with our own metric and recommendations.  We believe that using credit score only may be too restrictive to customers who will pay appropriately.  We are actively looking at the risk associated with these lines of credit.   For right now these loans are for a fixed credit limit.**

####Scenario 1  
**One stakeholder is interested in different levels of risk of default/non-payment.  They have provided information on 5 levels of risk, of every individual for their opinion and would like you to create a model based on this.  The 5 levels of risk, are "extremely low risk", "low risk", "moderate risk", "high risk", and "extremely high risk".  They ask you to find relevant predictors for this metric and report a model they can use in an automated procedure to provide these recommendations on top of what they already have.  Recommend a statistical model that you can use and how you would evaluate this model relative to customer needs.  Are there issues with the method you've proposed?  How can you explain this to your client?**

A multinomial regression model is best to determine the appropriate risk levels and relevant predictors as they relate to a customer’s probability of being in default. The model is ordinal in nature, meaning there is a natural order that occurs through the distribution of default risk levels, either from high to low risk or low to high risk depending on the stakeholder’s preference. It is that comparison of risk levels that is most important both in ordinal regression as well as to our stakeholder in making decisions in extending credit to potential customers.

Probabilities for each risk level can then be assigned given that the total risk profile for an individual must equal 100%. Setting total risk to 100%, or 1.0 in a statistical sense, allows us to then set each categorical variable to zero and create the baseline category representation. It is this representation in the model that allows us to compare each relevant predictor to those categories and develop trends and predictions of the informational criteria that show the highest probability of risk at each level by individual.

Having the ability to say the odds of moving from one risk level to another increase by a certain amount or the percentage of risk within the same risk level decreases as the informational criteria increases provides the stakeholder with the ability to automate or adjust its decision based on correlation of the attributes that interest them most.

However, this automation or preference cannot be done straight by the statistical outputs as the stakeholder must consider how to address false positives; that is individuals who may show as extremely high risk in the model but end up not defaulting on credit that is extended. If the company only extends credit to low-risk customers, it will limit its overall potential customer base. In these cases, it is recommended that additional environment factors and/or the total financial relationship and individual has with the company is taken into consideration when making the final decision on extending credit.   


####Scenario 2  
**Another stakeholder is interested in only defaults.  Since credit is offered to those who the company deems appropriate to offer credit to there is a certain "filter" on who will default.  Basically defaults become a "rare" occurrence, but we still want to be able to predict.  The data the company provides shows a 15% default rate, which they deem acceptable, but they want to see if they can identify contributory factors to defaults.  Prediction at an individual level is done through another means.  They want you to develop a statistical model that can identify the number of defaults that can occur in a specific month.  They also suggest there could be a time effect (month may matter) and number of defaults the previous month may matter as well.  Explain the model you would try to implement and why it makes sense.  How would you evaluate this model and method?  What are the issues that can arise with your approach?  How would you explain this to your client?**

A two-phase model utilizing binary logistic regression and time series analysis is implemented to evaluate credit offers and determine if potential customers are at risk to default on loans. This model is the best approach due to the dependent variable we are using for predicting only having two possible values – default or no default – as well as meeting the need to determine if seasonality impacts overall default volume.

In constructing the first part of the model, data is randomized to remove any bias of the independent variables that have the potential to lead to a mislabeling of customers against one of the two default values. The model investigates the potential predictor variables available to the stakeholder such as income, employment history, and credit score amongst others. Scatterplot jittering models are created to provide evaluation as a graphical depiction of the relationship between the potential predictor variable and possibility of default. Next, the model is more fully developed utilizing the GLM function with a family value set equal to binomial to create the output for determining the probability of a customer defaulting on credit for each predictor variable as others are held constant. Lastly, a check of confidence intervals is conducted against t-tests to confirm significance of each variable investigated.

The second part of the model, a scan function is used to import the data related to variables such as credit history, payment history, and current credit liability for those customers who have defaulted in the past. Once imported, a time series is created that investigates the frequency of customer payments relative to their due date, overall credit score on a monthly basis, and their debt-to-income ratio on a monthly basis over a period of time determined by the financial institution (e.g. three years). It is worth nothing that this pre-filtering of data to focus on specific variables and those already in default could create an issue with bias towards their undesirable behaviors. This possibility should be noted and adjusted as additional iterations of the model are executed with new variables and characteristics.

Next, plots of the dataset and log function of the dataset provide a visual representation of quantity and occurrence of when defaults tend to happen most, to include on specific months of interest by the stakeholder. These investigations are important as we want to identify areas where seasonality of default activity may occur, as well as how those customers who default in one month respond in subsequent months. Lastly, as trends in the default data are determined, the model will introduce a decomposition function to smooth out any seasonality and create an estimate on a month-by-month basis that can be referenced while also removing systematic error and accepting the randomness that comes with any dataset of activities.

In sum, the two-phase model provides the stakeholder with multiple levels of analytic rigor to make decisions on who to extend credit based on known variables of her choosing and, once extended, to adjust future credit limits or determine provisions of those customers who default in the future on an individual basis that is not impacted by seasonality. 

### Problem 3 (20 points)
**In this course we've discussed generalized linear models, time series analysis, the non-linear models.  In your own words and in less than a page describe how these three concepts are relate to one another?  If there is no relationship between these methods explain why they are not connected?**

Generalized Linear Models (GLMs), Non-Linear Models (NLMs), and Time Series Models (TSMs) are forms of regression analysis that are related in that all three are developed to predict the probability of an event that occurred, as related to a dependent variable, was indeed significant.  Thus, it provides a better understanding of how the data of interest is behaving overall. This enhanced insight of the data’s behavior helps decision makers and/or end users to then forecast those events that are statistically proven to most likely occur – or not occur - in the future and hopefully improve the end state that is desired.

Additionally, all three models depend on a defined data structure to enable the possibility of fitting the output in the most appropriate way to be impactful. For GLMs, the structure includes linear predictors and a relationship between the mean and variance of the population; for NLMs, the structure includes determining the proper model to use and requires starting values – typically in the form of minimums or maximums – before transformation can occur; and for TSMs the data must be defined at fixed intervals and typically in a specific order prior to conducting analysis of any kind. 

The three methods of interest do lack relation on a couple of key points that differentiate their value for potentially being utilized as a best-fit model for data behavior or prediction. The biggest of these is obvious in that NLMs do not require the relationship between variables nor does the data have to be linear in nature. Unlike linear regression and time series analysis, non-linear functions can have more than one parameter per predictor variable. This enhanced level of variation fractures NLM’s relationship to the other two regression models. It does afford the data scientist with far greater flexibility in using NLMs to determine the best fit possible for the data population. However, that flexibility takes away the ability to have one true hypothesis test that can be measured via P-value that is so commonly used in regression models to predict the validity of a trend or assumption. 


### Problem 4 (20 points)
**In this problem we want to study the labor participation of women in the 1980's.  The `Mroz` data in the `car`package depicts labor force participation.  Look at the help file for definition of the variables.  We are going to use `lfp` as our response in this case.  Create and decide on a model (you need to fit a few), that models this, discuss relevant variables and the impact they have.  You can use lwg in your model as it depicts what income would be expected if that individual worked. At least one of your models must, use splines or non-linear terms.**  

Start by reviewing the variables for the 753 observations of data.
```{r}
library(car)
library(splines)
library(mgcv)
data("Mroz")
head(Mroz)
help(Mroz) #help define the data
summary(Mroz) #overview of the data
sapply(Mroz, class) #see the variables classes
```
There are 325 observations where married women were not in the labor force and 428 observations where they were.  The response in our model is going to be a factor of "yes" or "no" for lfp.  The data is quite tidy and does not have "NA" values that need to be worked around.  We need to build a non-linear model that can consider the variables and make a prediction about whether lfp will be "yes" or "no".


####Build and hone basic models
```{r}
##Start by looking at a linear model with a binomial response.
modLFP1 <- glm(lfp ~ k5 + k618 + age + wc + hc + lwg + inc, data = Mroz, family = binomial(link = "logit"))

#Create the non-linear model, starting with polynomials:
##I tried all of the numeric values to different degrees of polynomials and used AIC to determine best fit.  The model below has the best polynomial results that I could put together.
modLFP2 <- glm(lfp ~ poly(k5,1,raw=FALSE) + poly(k618,1,raw=FALSE) + poly(age,1,raw=FALSE) + wc + hc + poly(lwg,2,raw=FALSE) + poly(inc,2,raw=FALSE), data = Mroz, family = binomial(link = "logit"))

#Compare each variable with a spline instead to see what can be improved.  Overall the splines improved the fit from the linear model, but not quite as well as the polynomial did.  One again, many variations of the split model below were tried before choosing this one.
modLFP3 <- glm(lfp ~ k5 + k618 + age + wc + hc + bs(lwg, knots =1)  + bs(inc,knots = c(25,50,75)), data = Mroz, family = binomial(link = "logit"))

#Create a GAM.  By adjusting the values of this and reviewing the AIC, the model below fit best.
modLFP4 <- gam(lfp ~ k5 + k618 + s(age) + wc + hc + s(lwg) + s(inc), data = Mroz, family = binomial(link = "logit"))

AIC(modLFP1,modLFP2, modLFP3, modLFP4)
summary(modLFP4)
```


Predict using the model and review how well it predicts the lfp.
```{r}
predLFP <-predict(modLFP4,type="response", newdata=Mroz)
predLFP1 <- ifelse(predLFP>=0.5, "Yes", "No")

table(predLFP1) #What the model predicts
table(predLFP1,Mroz$lfp) #This will show how many times the model predicted corrrectly against the actual data.

correctPreds <- as.numeric(length(predLFP1)) - (sum(table(predLFP1,Mroz$lfp))-sum(diag(table(predLFP1,Mroz$lfp)))) #How many predictions are correct.
print(correctPreds)
print(correctPreds / length(predLFP1)) 
```

The model that we fit was able to correctly predict 571 out of the 753 times (75%) whether a married woman within this survey would be part of the labor force.  From the summary, it is important to review the coefficients of the variables.  K618 (how many children aged 6-18 years old) and whether or not either gender went to college was not found to be significant within the model.  K5 (how many children aged 5 years old or younger) was significant in the model with a linear trend interfering the higher number of children in this age bracket. This makes it less likely for a woman to be in the workforce.

Age, lwg, and income were significant as well, but these trends were non-linear and so they lose simple interpretability.  To compensate for this, they are plotted individually below, but using the GAM of the model.

```{r}
library(faraway)
#Overview
plot(modLFP4, residuals=T, pch=19, cex=0.75,scheme=1, col=3 , shade=T, shade.col='gray')

#Plot model for age
modLFPAge <- gam(lfp ~ s(age), data = Mroz, family = binomial(link = "logit"))
plot(predLFP~age,data = Mroz)
x <-seq(30,60,1)
lines(x,ilogit(predict(modLFPAge,newdata=data.frame(age=x))),col=2)

#Plot model for lwg
modLFPLwg <- gam(lfp ~ s(lwg), data = Mroz, family = binomial(link = "logit"))
plot(predLFP~lwg,data = Mroz)
x <-seq(0,2.4,0.1)
lines(x,ilogit(predict(modLFPLwg,newdata=data.frame(lwg=x))),col=2)

#Plot model for inc
modLFPInc <- gam(lfp ~ s(inc), data = Mroz, family = binomial(link = "logit"))
plot(predLFP~inc,data = Mroz)
x <-seq(0,60,1) #not using full data range as it becomes too sparse
lines(x,ilogit(predict(modLFPInc,newdata=data.frame(inc=x))),col=2)
```

These plots help to visualize the non-linear trends within the model by breaking down the response trend based on a single variable. Findings include:
-Age stays pretty steady from 30 to 40 and then it starts to become less likely that the the married woman will participate in the workforce as their age increases.
-Lwg is hard to interpret but appears participatants in the survey with "average" levels of income potential were less likely to work.
-Inc shows that as the family income increases, the likelihood of a woman participating in the workforce decreases.


### Problem 5 (20 points)
**In the `mlogit` package there is a data set called `Fishing`.  This data depicts a customer choice model of different recreational fishing choices.  This is based on price, how many fish are caught, and income. Create a model based on price, catch, and inocme, that tells me about a customers choice of mode. Describe how you arrived at your model and any insights it provides.**

```{r echo = FALSE, warning = FALSE, message = FALSE}
#install.packages("mlogit")
#install.packages("groupdata2")
#install.packages("corrplot")

library(mlogit); library(tidyr); library(dplyr); library(stargazer); library(ggplot2)
library(groupdata2) #for partitions and to generate folds 
library(corrplot) #for correlation plots
library(nnet)
```


```{r}
?Fishing

data("Fishing")
head(Fishing)
```
 
This problem requires us to classify the mode of recreational fishing given the predictor variables. We have 4 choices for modes of recreation fishing: beach, pier, boat and charter (the choice); 2 alternative variables: price and catch rate for each mode (the alternatives); and income which indicates each individual's monthly income.  

We check the choices (response variable), and a charter trip appears to be the preferred choice with the highest number of individuals. 

```{r}
#check the dependent variable
table(Fishing$mode)
```

The data is in `wide` format; to use the mlogit function we first need to convert it into a `long` format
```{r}
fish_long <- mlogit.data(Fishing, shape = "wide", varying = 2:9, choice = "mode")
head(fish_long, 20)
```
In the `long` format we see that individuals are labeled `chid` or choice ID and each individual has 4 choices of mode. The data varies by both individuals and the fishing mode. The income does not vary by individual. `mode` represents the choice actually made by the individual. 

*Exploration of the data* 
```{r}
#summary of the fish_long data
summary(fish_long)
```

```{r}
#calculate proportion of mode choices 
exp <- group_by(fish_long, mode, alt) %>% 
        summarise(count = n()) %>% 
        mutate(etotal = sum(count), proportion = count/etotal)

#plot 
ggplot(exp, aes(x = alt, y = proportion, group = mode, linetype = mode, col = mode)) + 
        geom_line() + 
        ylim(0, 0.4) +
        ggtitle("Proportion of recreation mode, one of: beach, pier, boat and charter ") +
        xlab("Choice") + ylab("Proportion")
```

The actual choice made by an individual is given by the column `mode` as being TRUE, for example, the choice for individual `chid` 1 was `charter` and the choice for individual `chid` 3 was boat. From the plot we see that charter is the most chosen mode of recreational fishing with almost 40% of the total, followed by boat, pier and beach last with just over 10%. Conversely, we could have said that beach is the least selected alternative for recreational fishing, followed by pier, boat and charter and these correspond to the FALSE `mode`. 

We can now look at the price and catch rate distributions for each mode.
```{r}
#density plot for price
ggplot(fish_long, aes(x = price, col = alt)) + 
        geom_density() + 
        ggtitle("Price distribution for each mode choice") +
        xlab("Price") + ylab("Density")

#density plot for catch rate
ggplot(fish_long, aes(x = catch, col = alt)) + 
        geom_density() + 
        ggtitle("Catch rate distribution for each mode choice") +
        xlab("Catch rate") + ylab("Density")
```
  
We can make some interesting observations from the price density plots: first, we see skewed distributions for all the options, and all options have about the same range of prices although one can find more cheaper options for boat and charter, which makes sense since these are the most popular options (as we saw from the proportion plot). 

The catch rate density plots are less insightful but we can see that charter has the widest and the highest catch rate and it's a pretty uniform distribution. 

There doesn't seem to be a correlation between price and catch rate and we can verify by looking at a correlation plot:
```{r}
#subset the numerical variables only 
fish_num <- names(fish_long)[c(2, 4, 5)]
fish_num <- subset(fish_long, select = fish_num)

#calculate and plot correlations
m <- cor(fish_num)
corrplot(m, method = "number", type = "upper")
```
  
As suspected, there isn't much correlation between price and catch rate but we see a strong correlation between price and income. 

```{r}
#plot income vs price
ggplot(fish_long, aes(x = income, y = price, col = alt)) +
        geom_point() +
        geom_smooth() +
        ggtitle("Income vs price of mode choice") +
        xlab("Income") + ylab("Price")
```
  
From the plot we can see that as individual income increases, individuals are willing to pay higher prices for the mode of recreation. Individuals are willing to pay the most for the pier option even though the catch rate for this choice is much lower compared to charter or beach. 

```{r}
filter(fish_long, mode == TRUE) %>% 
        ggplot(aes(x = alt, y = income, col = alt)) + 
        geom_boxplot() +
        ggtitle("Income for each one of mode choices") +
        xlab("Mode choice") + ylab("Income")
```
  
Finaly we do not see a difference in income ranges for each of the mode choices.

We next summarize some of the data: 
```{r}
filter(fish_long, mode == TRUE) %>% 
        group_by(mode = alt) %>% 
        summarise(count = n(), min_price = min(price), max_price = max(price), med_price = median(price), mean_income = mean(income)) 

filter(fish_long, mode == TRUE) %>% 
        group_by(mode = alt) %>% 
        summarise(count = n(), min_catch = min(catch), max_catch = max(catch), med_catch = median(catch), mean_income = mean(income)) 
```

####Data Analysis

We will first use the mlogit package to model multinomial logit models:
```{r}
#start with the price and catch predictors and without the intercepts
m <- mlogit(mode ~ price + catch | 0, fish_long)

#model summary
summary(m)
```
The coefficient for price is negative, meaning that as the cost of the choice goes up keeping the catch rate the same, the probability of selecting the mode goes down. The coefficient for catch is positive indicating that as the rate of catch goes up, with price remaining constant, the probability of selecting that mode goes up. 

Both coefficients are significant with `p-value` being much smaller than 0.05

We compare the frequencies of the actual alternatives with frequency of model estimates:
```{r}
#calculate and extract both frequencies
freq_actual <- m$freq/sum(m$freq)
freq_model <- apply(fitted(m, outcome=FALSE), 2, mean)

#rbind
rbind(freq_actual, freq_model)
```
The model predictions are somewhat off from the actual frequencies of mode. The model is correctly predicted that `boat` and `charter` are the most selected choices although the model predicts that `boat` is more popular than `charter` which is not the case for actuals.

```{r}
#calculate the ratio of coefficients 
coef(m)["catch"]/coef(m)["price"]
```
The ratio of catch to price coefficient indicates that everything else being the same, individuals are willing to forgo 47% of the catch rate for a 1 unit reduction of mode price. This seems high. 

We will add the intercepts to see if it changes the model results:
```{r}
#with the intercepts
m1 <- mlogit(mode ~ price + catch, fish_long)

#model summary
summary(m1)
```
```{r}
#calculate and extract both frequencies
freq_model_m1 <- apply(fitted(m1, outcome=FALSE), 2, mean)

#rbind
rbind(freq_actual, freq_model_m1)
```

We see that the frequencies match exactly indicating an improvement to the model.

```{r}
#calculate the ratio of coefficients 
coef(m1)["catch"]/coef(m1)["price"]
```
The ratio of catch to price coefficient indicates that everything else being the same, individuals are willing to forgo 15% of the catch rate for a 1 unit reduction of mode price. This looks more reasonable than the previous estimate of 47%.

We will now add the income to the model. We will create 2 models: one with cost/income to model the fact that higher income individuals are less concerned with higher prices and vice-versa; and one with income effects on the alternatives. 
```{r}
#model for cost/income 
m2 <- mlogit(mode ~ price + catch + I(price/income), fish_long)
summary(m2)
```
```{r}
#model for income specific effects 
m3 <- mlogit(mode ~ price + catch | income, fish_long)
summary(m3)
```

The model with price/income was worse than our previous model where income was not taken into account even though the price/income is highly significant. The income specific model is an improvement and suggests that as income goes up the probability of boat choice goes up relative to the other alternatives. Also as income goes up the probability of the other choices goes down. The income terms are not significant indicating that monthly income doesn't have an effect on the choices of mode. This may be due to the fact that individuals may save a portion of their income for recreational fishing regardless of how much monthly income they make. 

We compare the models:
```{r}
#likelihood test 
lrtest(m3, m1)

#Wald test 
waldtest(m3, m1)

#score test 
scoretest(m3, m1)
```
All tests indicate that we should use the model with the income effect. 
```{r}
#convert the coefficients to odds 
exp(coefficients(m3)[4:8])
```
* For one unit increase in income, the odds of selecting charter and pier go down and the odds of selecting boat over beach go up.  
* For one unit increase in price, the odds of choosing a mode goes down by 0.98 assuming catch remains the same across all choices
* And for one unit increase in catch rate, the odds of choosing a mode goes up by 1.43 assuming price remains the same across all choices

```{r}
#prediction vs actuals --- was unable to create a contingency matrix
predictions <- predict(m3)
actuals <- m3$freq/sum(m3$freq)

rbind(predictions, actuals)
```

We will now try a multinomial logit model using the multinom() function. We will follow Faraway's procedure:
```{r}
#fit the model using all the relevant predictors
m4 <- multinom(alt ~ price + catch + income, fish_long)
```

We then use AIC to select which predictors add information to the model:
```{r}
#use the step function 
m5 <- step(m4)
```
The step function suggests that keeping all predictors results in the best AIC model, indicating that all variables are significant.

```{r}
#model summary
summary(m4)
```
The summary output has the coefficients and the standard errors. The coefficients are being compared to the baseline alt = beach.  
* A one unit increase in monthly income increases the log odds of choosing boat and charter over beach, but decreases the log odds of choosing pier over beach. The coefficients are quite small though which is consistent with our previous observation that income ranges are about the same for all mode choices.
* A one unit increase in price decreases the log odds of choosing beach over boat and charter and increases the log odds of selecting beach over pier - we saw in the density plots that pier prices remain higher than all other options for price greater than $150 so this result makes sense.  
* Catch rate has the highest coefficients and we can see why charter is the most popular choice since a one unit increase in catch rate increases the log odds of choosing charter over beach by 2 units.  
* We can infer from the model coefficients that catch rate is the biggest factor in selection of mode of recreational fishing, followed by price and then income. We previously saw that charter is the most popular mode and we can confirm from the model that the higher proportion of charter choice is significant.

We will now compare the predicted values from the model with the actuals:
```{r}
#prediction vs actuals 
xtabs(~ predict(m4) + fish_long$alt)
```
```{r}
#calculate the proportion of correct classifications 
(281 + 696 + 657 + 522) / nrow(fish_long)
```
Only 46% of the data is correctly classified based on fitting the entire data set. Had we split the data into training and testing subsets, we could expect the performance to be slightly worse since the model would be predicting new data. According to Faraway this performance is not unexpected when using multinomial logit models for classification problems. 
